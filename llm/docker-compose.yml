version: "3.9"

services:
  llama-cpp:
    image:  ghcr.io/ggerganov/llama.cpp:full
    container_name: llama.cpp
    restart: unless-stopped
    network_mode: host
    command: -s -m $LLM_MODEL_PATH --host $LLM_HOST --port $LLM_PORT -ngl $LLM_NGL --ctx-size $LLM_CTX_SIZE
    volumes:
      - ${VOLUMES_PATH}/llm/models:/models
